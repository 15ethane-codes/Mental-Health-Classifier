{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12258575,"sourceType":"datasetVersion","datasetId":7724616},{"sourceId":12258627,"sourceType":"datasetVersion","datasetId":7724651}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preprocessing","metadata":{"id":"DU-r4aexq650"}},{"cell_type":"code","source":"import pandas as pd\n\nmental_subs = ['depression', 'healthanxiety', 'suicidewatch', 'ptsd', 'bipolarreddit', 'mentalhealth']\ncontrol_subs = ['parenting', 'relationships', 'jokes', 'fitness', 'teaching', 'conspiracy']\n\ndataframes = []\n\nfor sub in mental_subs + control_subs:\n  df = pd.read_csv(f\"./{sub}_2019_features_tfidf_256.csv\")\n\n  df.rename(columns={'post': 'text'}, inplace=True)\n  df = df[df['text'].str.strip().astype(bool)] #rename empty posts-just in case\n\n  df['subreddit'] = sub\n  df['label'] = 1 if sub in mental_subs else 0\n\n  dataframes.append(df[['text', 'label', 'subreddit']])\n\ndf_all = pd.concat(dataframes, ignore_index=True)\ndf_all = df_all.sample(frac=1, random_state=42)\ndf_all.to_csv(\"bert_reddit_mental_health_2019.csv\", index=False)\n\nprint(df_all['label'].value_counts())","metadata":{"id":"eRxe-3vnTmv0","outputId":"95434342-95f4-4e25-d7b4-dce90cc19933"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef clean_text(text):\n    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)           #remove URLs\n    text = re.sub(r\"<.*?>\", \"\", text)                     #remove HTML tags\n    text = re.sub(r\"[^A-Za-z0-9\\s.,!?']\", \" \", text)      #keep basic punctuation\n    text = re.sub(r\"\\s+\", \" \", text)                      #remove extra spaces\n    return text.strip().lower()                           #strip and lowercase\n\ndf_all['text'] = df_all['text'].astype(str).apply(clean_text)","metadata":{"id":"qnlDFIVAoTX-"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.utils import resample\n\ndf_majority = df_all[df_all['label'] == 0]\ndf_minority = df_all[df_all['label'] == 1]\n\ndf_majority_downsampled = resample(df_majority,\n                                   replace=False,\n                                   n_samples=len(df_minority),\n                                   random_state=42)\n\ndf_balanced = pd.concat([df_majority_downsampled, df_minority])\ndf_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n\ndf_balanced.to_csv(\"bert_clean_balanced_reddit.csv\", index=False)\nprint(df_balanced['label'].value_counts())","metadata":{"id":"ne_1xQ00oxqH","outputId":"dd1a1925-eeb0-45e9-b4d3-bc75bb32e5fc"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BERT Training","metadata":{"id":"eFgvk7K0q_Lr"}},{"cell_type":"code","source":"!pip install transformers datasets scikit-learn\n!pip install --upgrade transformers","metadata":{"id":"09foe8MJqsb8","outputId":"9dc6ad83-d42f-4fb7-dca2-0a3b97ca98e0","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T15:16:23.797151Z","iopub.execute_input":"2025-07-04T15:16:23.797590Z","iopub.status.idle":"2025-07-04T15:16:44.436411Z","shell.execute_reply.started":"2025-07-04T15:16:23.797566Z","shell.execute_reply":"2025-07-04T15:16:44.435199Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nCollecting transformers\n  Downloading transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.53.1-py3-none-any.whl (10.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.3\n    Uninstalling transformers-4.51.3:\n      Successfully uninstalled transformers-4.51.3\nSuccessfully installed transformers-4.53.1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import transformers\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer\nfrom datasets import Dataset\n\nprint(transformers.__version__)\ndf = pd.read_csv(\"/kaggle/input/redditmentalhealth-dataset/bert_clean_balanced_reddit.csv\")\n\ndf_0 = df[df['label']==0].sample(n=60000,random_state=42)\ndf_1 = df[df['label']==1].sample(n=60000,random_state=42)\ndf_small = pd.concat([df_0,df_1]).sample(frac=1,random_state=42).reset_index(drop=True)\ndf_small.to_csv(\"bert_small_10.csv\", index=False)\n\ndf_1 = pd.read_csv(\"bert_small_10.csv\")\n\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df_1['text'].tolist(), df_1['label'].tolist(), test_size=0.2, random_state=42)\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=256)\n\ntrain_dataset = Dataset.from_dict({\n    'input_ids': train_encodings['input_ids'],\n    'attention_mask': train_encodings['attention_mask'],\n    'labels': train_labels\n})\ntest_dataset = Dataset.from_dict({\n    'input_ids': test_encodings['input_ids'],\n    'attention_mask': test_encodings['attention_mask'],\n    'labels': test_labels})\n","metadata":{"id":"8TKxhTgQrWrn","outputId":"a1cc9a2b-1181-4496-ac74-8388b7c84472","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T00:05:35.392254Z","iopub.execute_input":"2025-07-07T00:05:35.392759Z","iopub.status.idle":"2025-07-07T00:11:30.726845Z","shell.execute_reply.started":"2025-07-07T00:05:35.392735Z","shell.execute_reply":"2025-07-07T00:11:30.726274Z"}},"outputs":[{"name":"stdout","text":"4.51.3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9a3464fc11b4242833945ccb6d77c4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3985e07e3614c04bb9baf2331396888"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b67b258e742d4a08aed8322bc5e2f074"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7415d02c4734258a7e526fe4e940ae2"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from transformers import TrainingArguments\nprint(TrainingArguments.__module__)\nhelp(TrainingArguments)","metadata":{"id":"idr3gdGA-uCU","outputId":"08d92b4c-5b06-4a63-937b-6f452fded355"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport os\nfrom transformers import BertForSequenceClassification, Trainer, TrainingArguments, EvalPrediction, EarlyStoppingCallback\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom transformers import BertConfig\n\nconfig = BertConfig.from_pretrained(\"bert-base-uncased\", hidden_dropout_prob=0.3, attention_probs_dropout_prob=0.3, num_labels=2)\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config)\n\nfor name, param in model.bert.named_parameters():\n    if any(f\"layer{i}.\" in name for i in range(8)):\n        param.requires_grad = False\n\nearly_stopping = EarlyStoppingCallback(early_stopping_patience=1)\ndef compute_metrics(p: EvalPrediction):\n  preds = torch.argmax(torch.tensor(p.predictions), axis=1)\n  labels = torch.tensor(p.label_ids)\n  precision, recall, f1, _ = precision_recall_fscore_support(labels,preds, average='binary')\n  acc = accuracy_score(labels, preds)\n  return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\ntraining_args = TrainingArguments(\n    output_dir=\"./bert_mh_results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    warmup_steps=500,\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    save_total_limit=2,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    learning_rate = 2e-5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[early_stopping]\n)\n\ntrainer.train()","metadata":{"id":"4CJwOQWll52M","outputId":"6bafca9e-570e-4b43-ed95-e31ea12b8472","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T00:11:30.727955Z","iopub.execute_input":"2025-07-07T00:11:30.728505Z"}},"outputs":[{"name":"stderr","text":"2025-07-07 00:11:35.191168: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751847095.674142      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751847095.796065      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"447586c3422a4d0c84a395c235ae3043"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6255' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6255/9000 1:35:46 < 42:02, 1.09 it/s, Epoch 2.08/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.119700</td>\n      <td>0.087362</td>\n      <td>0.972375</td>\n      <td>0.973583</td>\n      <td>0.971066</td>\n      <td>0.972323</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.083900</td>\n      <td>0.089921</td>\n      <td>0.973083</td>\n      <td>0.961523</td>\n      <td>0.985575</td>\n      <td>0.973400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/bert_mental_health_model\")\ntokenizer.save_pretrained(\"/kaggle/working/bert_mental_health_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\npreds = trainer.predict(test_dataset)\ny_pred = np.argmax(preds.predictions, axis=1)\ny_true = preds.label_ids\n\nprint(classification_report(y_true,y_pred))\n\n#7 points \ntrain_loss = [log['loss'] for log in trainer.state.log_history if 'loss' in log and 'epoch' in log]\ntrain_epochs = [log['epoch'] for log in trainer.state.log_history if 'loss' in log and 'epoch' in log]\n#3 points\neval_loss = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log and 'epoch' in log]\neval_epochs = [log['epoch'] for log in trainer.state.log_history if 'eval_loss' in log and 'epoch' in log]\n\nplt.plot(train_epochs, train_loss, label='Train Loss', marker='o')\nplt.plot(eval_epochs, eval_loss, label='Eval Loss', marker='x')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"BERT Training vs. Evaluation Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from wordcloud import WordCloud\n\ncm = confusion_matrix(y_true, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Neutral\", \"Mental Health\"])\ndisp.plot(cmap=\"Blues\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\nmh_text = df_1[df_1['label'] == 1]['text'].str.cat(sep=' ')\nneutral_text = df_1[df_1['label'] == 0]['text'].str.cat(sep=' ')\n\nmh_wc = WordCloud(width=800, height=400, background_color='white').generate(mh_text)\nneutral_wc = WordCloud(width=800, height=400, background_color='white').generate(neutral_text)\n\nplt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nplt.imshow(mh_wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title(\"Mental Health Class\")\n\nplt.subplot(1,2,2)\nplt.imshow(neutral_wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title(\"Neutral Class\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}